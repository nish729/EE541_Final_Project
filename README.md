# EE541_Final_Project

## Motivation:
The ability to hear is a sense that most people take for granted. Hearing is useful for many tasks: listening to music, being aware of ones' surroundings, listening to the beauty of nature, etc. However, the loss of/hindering of this sense is a major obstacle to one integral element of daily life: communication. There are approximately 1.5 billion people in the modern age that are deaf or are hard of hearing (DHH). These people primarily communicate through sign language. We will narrow our focus to ASL (American Sign Language), seen as the easiest sign language to learn. There are two types of communication via sign language: fingerspelling and gesturing. Fingerspelling is explicity spelling a word via letters, whereas gesturing is both spatial and temporal and conveys tone, intonation, and emotion. We further narrow our scope to only fingerspelling. This allows DHH people to communicate with themselves, but it still presents an issue when someone with good hearing attempts to interact with DHH people. This is the issue we are trying to mitigate. By training a deep learning model to accurately classify sign language into letters, we can get a better idea of what DHH people are attempting to communicate.  

## Dataset:
The original dataset was developed by Akash Nagaraj and can be found on [Kaggle](https://www.kaggle.com/datasets/grassknoted/asl-alphabet). It is 1.03 GB and consists of 87,000 200x200 RGB training images as well as 29 200x200 RGB testing images. The dataset contains 29 classes: ASL letters A-Z, Space, Delete, and Nothing.

## Getting Started:
Clone the repository and get the dataset from the above link. The improved dataset can be provided upon request.
