# EE541_Final_Project

# Motivation
The ability to hear is a sense that most people take for granted. Hearing is useful for many tasks: listening to music, being aware of ones' surroundings, listening to the beauty of nature, etc. However, the loss of/hindering of this sense is a major obstacle to one integral element of daily life: communication. There are approximately 1.5 billion people in the modern age that are deaf or are hard of hearing (DHH) \cite{worldhealthorganization}. These people primarily communicate through sign language. We will narrow our focus to ASL (American Sign Language), seen as the easiest sign language to learn. There are two types of communication via sign language: fingerspelling and gesturing. Fingerspelling is explicity spelling a word via letters, whereas gesturing is both spatial and temporal and conveys tone, intonation, and emotion. We further narrow our scope to only fingerspelling. This allows DHH people to communicate with themselves, but it still presents an issue when someone with good hearing attempts to interact with DHH people. This is the issue we are trying to mitigate. By training a deep learning model to accurately classify sign language into letters, we can get a better idea of what DHH people are attempting to communicate.  


# Objective
This paper seeks to compare the effectiveness of 5 Deep Learning models on classifying ASL letters: a custom MultiLayer Perceptron network, a custom CNN network, ResNet18, MobileNetV2, and SqueezeNet. These networks were chosen to show how a baseline (custom MLP), typical CNN (custom CNN), and three modern networks (ResNet18, MobileNetV2, and SqueezeNet) perform with this classification task. Preliminary research showed the latter three to have high accuracy with general image classification. We will train each model and tune them independently, with varying optimizers and hyperparameters.
